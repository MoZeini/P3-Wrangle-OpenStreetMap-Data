{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenStreetMap Data Case Study (Boston)\n",
    "by Mohamad Zeini Jahromi\n",
    "\n",
    "## Introduction\n",
    "OpenStreetMap (OSM) is a collaborative project to create a free editable map of the world. The creation and growth of OSM has been motivated by restrictions on use or availability of map information across much of the world, and the advent of inexpensive portable satellite navigation devices.\n",
    "\n",
    "## Objective\n",
    "While very useful, OSM data can be quite messy at times. In this project, I will walk you through the cleaning process of the data, storing data in the CSV format and analyzing it via SQL queries. \n",
    "\n",
    "I live in a small college town and for the purpose of this study, I chose one of my favorite cities in united states, Boston. The XML file (414 MB) has been downloaded from [Map Zen website](https://mapzen.com/data/metro-extracts/metro/boston_massachusetts/).\n",
    "\n",
    "## Data wrangling\n",
    "The following steps have been taken in our data wrangling projects:\n",
    "\n",
    "- Creating a smaller size (4 MB) sample file from the original XML file.\n",
    "- Auditing the types of tags and attributes\n",
    "- Deciding which tags needs to be edited\n",
    "- Systematically checking for inconsistencies\n",
    "- Editing the inconsistent values\n",
    "- Saving the data in CSV format\n",
    "- Converting the CSV format to SQL Database\n",
    "- Analyzing the data using SQL queries \n",
    "\n",
    "## Types of tags and attributes\n",
    "I used the iterative parsing to process the map file and find out not only what tags are there, but also how many, to get the\n",
    "feeling on how much of which data I can expect to have in the map.\n",
    "The following code returns a dictionary with the tag name as the key and number of times this tag can be encountered in \n",
    "the map as value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node : 1931442\n",
      "nd : 2323539\n",
      "bounds : 1\n",
      "member : 10959\n",
      "tag : 900810\n",
      "relation : 1295\n",
      "way : 309066\n",
      "osm : 1\n"
     ]
    }
   ],
   "source": [
    "import xml.etree.cElementTree as ET\n",
    "import pprint\n",
    "\n",
    "# original OSM file and its sample\n",
    "OSM_FILE = \"boston_massachusetts.osm\"             \n",
    "SAMPLE_FILE = \"boston_massachusetts_sample.osm\"\n",
    "\n",
    "# This function counts the unique number of tags in the given file\n",
    "def count_tags(filename):                         \n",
    "    tags = {}\n",
    "    context = ET.iterparse(filename)\n",
    "    for event, elem in context:\n",
    "        if elem.tag not in tags.keys():\n",
    "            tags[elem.tag] = 1\n",
    "        else:\n",
    "            tags[elem.tag] +=1\n",
    "    return tags\n",
    "\n",
    "# Prints out the tags and their count numbers\n",
    "#tags = count_tags(SAMPLE_FILE)\n",
    "tags = count_tags(OSM_FILE)\n",
    "for k, v in tags.items(): print k, \":\", v\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results show that **osm** and **bounds** are the root elements and other tags are child. Moreover, the majority of the information are located in **node** and **way** tags. \n",
    "There are many different child tags in the file and since cleaning all of them will take a lot of time and for the purpose of this study, I decided to check the following tags attributes for consistency.\n",
    "- addr:street\n",
    "- addr:state\n",
    "- addr:postcode\n",
    "\n",
    "## Improving Street Names\n",
    "\n",
    "First we audit the **OSM FILE** to find the unexpected street types with respect to the appropriate ones in **expected list**.\n",
    "The following function identifies all the unique attributes in this tags.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of street types = 61\n",
      "6 : set(['South Station, near Track 6'])\n",
      "1302 : set(['Cambridge Street #1302'])\n",
      "Elm : set(['Elm'])\n",
      "St, : set(['Walnut St,'])\n",
      "Winsor : set(['Winsor'])\n"
     ]
    }
   ],
   "source": [
    "import xml.etree.cElementTree as ET\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import pprint\n",
    "\n",
    "# A regular expression to find the end word of address string which can includes \".\" \n",
    "street_type_re = re.compile(r'\\b\\S+\\.?$', re.IGNORECASE)\n",
    "\n",
    "# List of expected street types \n",
    "expected = [\"Street\", \"Avenue\", \"Boulevard\", \"Drive\", \"Court\", \"Place\", \"Square\", \"Lane\", \"Road\", \n",
    "            \"Trail\", \"Parkway\", \"Commons\", 'Circle','Highway','Center','Turnpike','Way']\n",
    "\n",
    "# This function creates a list of all unexpected street types \n",
    "# which are not in the expected list\n",
    "def audit_street_type(street_types, street_name):\n",
    "    m = street_type_re.search(street_name)\n",
    "    if m:\n",
    "        street_type = m.group()\n",
    "        if street_type not in expected:\n",
    "            street_types[street_type].add(street_name)\n",
    "\n",
    "# This function checks if the address type is a street type\n",
    "def is_street_name(elem):\n",
    "    return (elem.attrib['k'] == \"addr:street\")\n",
    "\n",
    "# The main function to audit the OSM file and returns the unexpected street types\n",
    "# and their respective examples\n",
    "def audit(osmfile):\n",
    "    osm_file = open(osmfile, \"r\")\n",
    "    street_types = defaultdict(set)\n",
    "    for event, elem in ET.iterparse(osm_file, events=(\"start\",)):\n",
    "\n",
    "        if elem.tag == \"node\" or elem.tag == \"way\":\n",
    "            for tag in elem.iter(\"tag\"):\n",
    "                if is_street_name(tag):\n",
    "                    audit_street_type(street_types, tag.attrib['v'])\n",
    "    osm_file.close()\n",
    "    return street_types\n",
    "\n",
    "# Print out the examples of unexpected street types\n",
    "#street_types = audit(SAMPLE_FILE)\n",
    "street_types = audit(OSM_FILE)\n",
    "print 'Number of street types =', len(street_types)\n",
    "for street_type, ways in street_types.items()[0:5]: print street_type, \":\", ways"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we create a **mapping list** to reflect the changes needed to fix the unexpected street types to the appropriate ones in the **expected list**. I have added items to mappings only for the actual problems I found in this **OSM FILE**, not a generalized solution, since that may and will depend on the particular area I'm auditing.\n",
    "\n",
    "Finally, the following code (**update_street** function) fixes the street names. The function takes a string with street name as an argument and return the fixed name. I have provided a simple test so that you see what exactly is expected.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "South Station, near Track 6 => South Station, near Track 6\n",
      "Cambridge Street #1302 => Cambridge Street #1302\n",
      "Elm => Elm\n",
      "Walnut St, => Walnut Street\n",
      "Winsor => Winsor\n"
     ]
    }
   ],
   "source": [
    "# Dictionary of unexpected street types as keys and their appropriate ones as values\n",
    "mapping_street = {\"Ave\": \"Avenue\",\"Ave.\":\"Avenue\",\"Ct\":\"Court\",\"Dr\":\"Drive\",\"Ext\":\"Exit\",\n",
    "           \"HIghway\":\"Highway\",\"Hwy\":\"Highway\",\"Pkwy\":\"Parkway\",\"Pl\":\"Place\",\"Rd\":\"Road\",\n",
    "           \"ST\":\"Street\",\"Sq.\":\"Square\",\"St\":\"Street\",\"St,\":\"Street\",\"St.\":\"Street\",\n",
    "           \"Street.\":\"Street\",\"rd.\":\"Road\",\"st\":\"Street\",\"street\":\"Street\"}\n",
    "\n",
    "# This function update street names using the \"mapping_street\" dictionary\n",
    "def update_street(name, mapping):\n",
    "    name = name.split(\" \")\n",
    "    if name[-1] in mapping.keys():\n",
    "        name[-1] = mapping[name[-1]]\n",
    "    name = \" \".join(name)\n",
    "    return name\n",
    "\n",
    "# Print out the examples of update function\n",
    "for street_type, ways in street_types.items()[0:5]:\n",
    "    for name in ways:\n",
    "        better_name = update_street(name, mapping_street)\n",
    "        print name, \"=>\", better_name\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improving State Names\n",
    "\n",
    "First we audit the **OSM FILE** to find all types of state names and their respective numbers.\n",
    "The following function identifies all the unique attributes in this tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of state types = 7\n",
      "ma : 6\n",
      "MA : 2029\n",
      "MA- MASSACHUSETTS : 60\n",
      "Massachusetts : 13\n",
      "Ma : 6\n",
      "WA : 1\n",
      "MASSACHUSETTS : 1\n"
     ]
    }
   ],
   "source": [
    "# This function creates a list of all types of states  \n",
    "def audit_state_type(state_types, state_name):\n",
    "    if state_name not in state_types:\n",
    "        state_types[state_name] = 1\n",
    "    else:\n",
    "        state_types[state_name] += 1\n",
    "\n",
    "# This function checks if the address type is a state type\n",
    "def is_state_name(elem):\n",
    "    return (elem.attrib['k'] == \"addr:state\")\n",
    "\n",
    "# The main function to audit the OSM file and returns all types of states\n",
    "# in the OSM file\n",
    "def audit(osmfile):\n",
    "    osm_file = open(osmfile, \"r\")\n",
    "    state_types = defaultdict(set)\n",
    "    for event, elem in ET.iterparse(osm_file, events=(\"start\",)):\n",
    "        if elem.tag == \"node\" or elem.tag == \"way\":\n",
    "            for tag in elem.iter(\"tag\"):\n",
    "                if is_state_name(tag):\n",
    "                    audit_state_type(state_types, tag.attrib['v'])\n",
    "    osm_file.close()\n",
    "    return state_types\n",
    "\n",
    "# Print out all types of states \n",
    "#st_types = audit(SAMPLE_FILE)\n",
    "state_types = audit(OSM_FILE)\n",
    "print 'Number of state types =',len(state_types)\n",
    "for k, v in state_types.items(): print k, \":\", v\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 7 different state types in our data file and only 1 case has 'WA' as state type and the rest belong to 'MA'.\n",
    "The following code will change all notations of Massachusetts state to 'MA'. The function takes a string with state name as an argument and return the fixed name. I have provided a simple test so that you see what exactly is expected.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ma => MA\n",
      "MA => MA\n",
      "MA- MASSACHUSETTS => MA\n",
      "Massachusetts => MA\n",
      "Ma => MA\n",
      "WA => WA\n",
      "MASSACHUSETTS => MA\n"
     ]
    }
   ],
   "source": [
    "# Dictionary of unexpected state types as keys and their appropriate ones as values\n",
    "mapping_state = { \"MA- MASSACHUSETTS\": \"MA\",\n",
    "            \"MASSACHUSETTS\": \"MA\",\n",
    "            \"Ma\": \"MA\",\n",
    "            \"Massachusetts\": \"MA\",\n",
    "            \"ma\": \"MA\"\n",
    "            }\n",
    "\n",
    "# This function update state names using the \"mapping_state\" dictionary\n",
    "def update_state(name, mapping):\n",
    "    if name in mapping.keys():\n",
    "        name = mapping[name]\n",
    "    return name\n",
    "\n",
    "# Print out the examples of update function\n",
    "for state_type, num in state_types.iteritems():\n",
    "    better_name = update_state(state_type, mapping_state)\n",
    "    print state_type, \"=>\", better_name\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improving ZIP Codes\n",
    "\n",
    "First we audit the **OSM FILE** to find the all notations of zip codes in our data file and their respective numbers.\n",
    "The following function identifies all the unique attributes in this tags.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of zipcode types = 123\n",
      "0239 : 1\n",
      "02186 : 9\n",
      "02184 : 3\n",
      "02134-1327 : 1\n",
      "02189 : 1\n"
     ]
    }
   ],
   "source": [
    "# This function creates a list of all types of zipcode  \n",
    "def audit_zipcode(zipcode_types, zipcode):\n",
    "    if zipcode not in zipcode_types:\n",
    "        zipcode_types[zipcode] = 1\n",
    "    else:\n",
    "        zipcode_types[zipcode] += 1\n",
    "\n",
    "# This function checks if the address type is a zipcode type        \n",
    "def is_zipcode(elem):\n",
    "    return (elem.attrib['k'] == \"addr:postcode\")\n",
    "\n",
    "# The main function to audit the OSM file and returns all types of zipcode\n",
    "# in the OSM file\n",
    "def audit(osmfile):\n",
    "    osm_file = open(osmfile, \"r\")\n",
    "    zipcode_types = defaultdict(set)\n",
    "    for event, elem in ET.iterparse(osm_file, events=(\"start\",)):\n",
    "\n",
    "        if elem.tag == \"node\" or elem.tag == \"way\":\n",
    "            for tag in elem.iter(\"tag\"):\n",
    "                if is_zipcode(tag):\n",
    "                    audit_zipcode(zipcode_types, tag.attrib['v'])\n",
    "    osm_file.close()\n",
    "    return zipcode_types\n",
    "\n",
    "# Print out all types of zipcode along with their cout numbers \n",
    "#st_types = audit(SAMPLE_FILE)\n",
    "zipcode_types = audit(OSM_FILE)\n",
    "print 'Number of zipcode types =',len(zipcode_types)\n",
    "for k, v in zipcode_types.items()[0:5]: print k, \":\", v\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After auditing the file, the following types of zip code are found:\n",
    "- 02446\n",
    "- 02445-5841\n",
    "- MA 02116\n",
    "- Unknown\n",
    "\n",
    "All these zip code types have at least 5 digits in their string. The following code will extract the first 5 digits from zip code string and then checks to see if the extracted zip code is located in Boston area or not.\n",
    "\n",
    "All the zip codes outside of Boston area and all the Unknown zip codes will be assigned as zero. I have provided a simple test so that you see what exactly is expected.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0239 => 0\n",
      "02186 => 02186\n",
      "02184 => 02184\n",
      "02134-1327 => 02134\n",
      "02189 => 02189\n"
     ]
    }
   ],
   "source": [
    "# A regular expression to find zipcodes (first five digits) within a string \n",
    "zipcode_re = re.compile(r'\\d+')\n",
    "\n",
    "# This function update zipcode using extracted digits from regular expression\n",
    "# It returns \"0\" if the zipcode was not found or if it is outside of Boston area.\n",
    "def update_zipcode(zipcode):\n",
    "    zipcode = zipcode_re.findall(zipcode)\n",
    "    \n",
    "    if zipcode != [] and len(zipcode[0]) == 5:\n",
    "        zipcode = zipcode[0]\n",
    "        if int(zipcode) <= 1431 or int(zipcode) >= 2770:\n",
    "            zipcode = '0'\n",
    "    else:\n",
    "        zipcode = '0'\n",
    "    return zipcode\n",
    "\n",
    "# Print out the examples of update function\n",
    "for zipcode_type, num in zipcode_types.items()[0:5]:\n",
    "    better_zipcode = update_zipcode(zipcode_type)\n",
    "    print zipcode_type, \"=>\", better_zipcode\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing CSV files for SQL Database\n",
    "\n",
    "After auditing is complete the next step is to prepare the data to be inserted into a SQL database. To do so I will parse the elements in the OSM XML file, transforming them from document format to tabular format, thus making it possible to write to .csv files.  These csv files can then easily be imported to a SQL database as tables.\n",
    "\n",
    "The process for this transformation is as follows:\n",
    "- Use iterparse to iteratively step through each top level element in the XML\n",
    "- Shape each element into several data structures using a custom function\n",
    "- Utilize a schema and validation library to ensure the transformed data is in the correct format\n",
    "- Write each data structure to the appropriate .csv files\n",
    "\n",
    "The shape_element function will transform each element into the correct format. I have used the Udacity pre-defined schema (**schema.py** file) for the .csv files and the eventual tables. And finally, using the **cerberus** library I validated the output against this schema to ensure it is correct.\n",
    "\n",
    "### If the element top level tag is \"node\":\n",
    "The dictionary returned should have the format {\"node\": .., \"node_tags\": ...}\n",
    "\n",
    "The \"node\" field should hold a dictionary of the following top level node attributes:\n",
    "- id\n",
    "- user\n",
    "- uid\n",
    "- version\n",
    "- lat\n",
    "- lon\n",
    "- timestamp\n",
    "- changeset\n",
    "All other attributes can be ignored\n",
    "\n",
    "The \"node_tags\" field should hold a list of dictionaries, one per secondary tag. Secondary tags are\n",
    "child tags of node which have the tag name/type: \"tag\". Each dictionary should have the following\n",
    "fields from the secondary tag attributes:\n",
    "- id: the top level node id attribute value\n",
    "- key: the full tag \"k\" attribute value if no colon is present or the characters after the colon if one is.\n",
    "- value: the tag \"v\" attribute value\n",
    "- type: either the characters before the colon in the tag \"k\" value or \"regular\" if a colon is not present.\n",
    "\n",
    "Additionally,\n",
    "\n",
    "- if the tag \"k\" value contains problematic characters, the tag should be ignored\n",
    "- if the tag \"k\" value contains a \":\" the characters before the \":\" should be set as the tag type\n",
    "  and characters after the \":\" should be set as the tag key\n",
    "- if there are additional \":\" in the \"k\" value they and they should be ignored and kept as part of\n",
    "  the tag key. \n",
    "- If a node has no secondary tags then the \"node_tags\" field should just contain an empty list.\n",
    "\n",
    "\n",
    "### If the element top level tag is \"way\":\n",
    "The dictionary should have the format {\"way\": ..., \"way_tags\": ..., \"way_nodes\": ...}\n",
    "\n",
    "The \"way\" field should hold a dictionary of the following top level way attributes:\n",
    "- id\n",
    "-  user\n",
    "- uid\n",
    "- version\n",
    "- timestamp\n",
    "- changeset\n",
    "\n",
    "All other attributes can be ignored\n",
    "\n",
    "The \"way_tags\" field should again hold a list of dictionaries, following the exact same rules as\n",
    "for \"node_tags\".\n",
    "\n",
    "Additionally, the dictionary should have a field \"way_nodes\". \"way_nodes\" should hold a list of\n",
    "dictionaries, one for each nd child tag.  Each dictionary should have the fields:\n",
    "- id: the top level element (way) id\n",
    "- node_id: the ref attribute value of the nd tag\n",
    "- position: the index starting at 0 of the nd tag i.e. what order the nd tag appears within the way element\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import codecs\n",
    "import pprint\n",
    "import re\n",
    "import xml.etree.cElementTree as ET\n",
    "import cerberus\n",
    "import schema\n",
    "\n",
    "#OSM_PATH = \"boston_massachusetts_sample.osm\"\n",
    "OSM_PATH = \"boston_massachusetts.osm\"\n",
    "\n",
    "# Pathes to save the CSV files\n",
    "NODES_PATH = \"nodes.csv\"\n",
    "NODE_TAGS_PATH = \"nodes_tags.csv\"\n",
    "WAYS_PATH = \"ways.csv\"\n",
    "WAY_NODES_PATH = \"ways_nodes.csv\"\n",
    "WAY_TAGS_PATH = \"ways_tags.csv\"\n",
    "\n",
    "# Regular expression to find tags with a colon in their names (lower_colon)\n",
    "# or tags with problematic characters (problemchars).    \n",
    "LOWER_COLON = re.compile(r'^([a-z]|_)+:([a-z]|_)+')\n",
    "PROBLEMCHARS = re.compile(r'[=\\+/&<>;\\'\"\\?%#$@\\,\\. \\t\\r\\n]')\n",
    "\n",
    "# The Udacity pre-defined schema to transform each element into the correct format. \n",
    "SCHEMA = schema.schema\n",
    "\n",
    "# the fields order in the csvs matches the column order in the sql table schema\n",
    "NODE_FIELDS = ['id', 'lat', 'lon', 'user', 'uid', 'version', 'changeset', 'timestamp']\n",
    "NODE_TAGS_FIELDS = ['id', 'key', 'value', 'type']\n",
    "WAY_FIELDS = ['id', 'user', 'uid', 'version', 'changeset', 'timestamp']\n",
    "WAY_TAGS_FIELDS = ['id', 'key', 'value', 'type']\n",
    "WAY_NODES_FIELDS = ['id', 'node_id', 'position']\n",
    "\n",
    "# The shape_element function will transform each element into the correct format. \n",
    "# using schema.py file and checks the format using the cerberus library \n",
    "# and their respective values using update functions.\n",
    "def shape_element(element, node_attr_fields=NODE_FIELDS, way_attr_fields=WAY_FIELDS,\n",
    "                  problem_chars=PROBLEMCHARS, default_tag_type='regular'):\n",
    "    \"\"\"Clean and shape node or way XML element to Python dict\"\"\"\n",
    "\n",
    "    node_attribs = {}\n",
    "    way_attribs = {}\n",
    "    way_nodes = []\n",
    "    tags = []  # Handle secondary tags the same way for both node and way elements\n",
    "\n",
    "    if element.tag == 'node':\n",
    "        node_attribs['id'] = element.attrib['id']\n",
    "        node_attribs['user'] = element.attrib['user']\n",
    "        node_attribs['uid'] = element.attrib['uid']\n",
    "        node_attribs['version'] = element.attrib['version']\n",
    "        node_attribs['lat'] = element.attrib['lat']\n",
    "        node_attribs['lon'] = element.attrib['lon']\n",
    "        node_attribs['timestamp'] = element.attrib['timestamp']\n",
    "        node_attribs['changeset'] = element.attrib['changeset']\n",
    "\n",
    "        for tag in element.iter(\"tag\"):\n",
    "            d={}\n",
    "            d['id'] = node_attribs['id']\n",
    "            k = tag.attrib['k']\n",
    "            if PROBLEMCHARS.match(k) == None:\n",
    "                if LOWER_COLON.match(k) != None:\n",
    "                    d['type'] = k.split(':')[0]\n",
    "                    d['key'] = ':'.join(k.split(':')[1:])\n",
    "                else:\n",
    "                    d['type'] = 'regular'\n",
    "                    d['key'] = k\n",
    "                    \n",
    "            if  k == \"addr:street\":\n",
    "                d['value'] = update_street(tag.attrib['v'], mapping_street)\n",
    "            elif  k == \"addr:state\":\n",
    "                d['value'] = update_state(tag.attrib['v'], mapping_state)\n",
    "            elif  k == \"addr:postcode\":\n",
    "                d['value'] = update_zipcode(tag.attrib['v'])\n",
    "            else:\n",
    "                d['value'] = tag.attrib['v']\n",
    "            tags.append(d)\n",
    "        return {'node': node_attribs, 'node_tags': tags}\n",
    "    \n",
    "    elif element.tag == 'way':\n",
    "        way_attribs['id'] = element.attrib['id']\n",
    "        way_attribs['user'] = element.attrib['user']\n",
    "        way_attribs['uid'] = element.attrib['uid']\n",
    "        way_attribs['version'] = element.attrib['version']\n",
    "        way_attribs['timestamp'] = element.attrib['timestamp']\n",
    "        way_attribs['changeset'] = element.attrib['changeset']\n",
    "\n",
    "        for tag in element.iter(\"tag\"):\n",
    "            d={}\n",
    "            d['id'] = way_attribs['id']\n",
    "            k = tag.attrib['k']\n",
    "            if PROBLEMCHARS.match(k) == None:\n",
    "                if LOWER_COLON.match(k) != None:\n",
    "                    d['type'] = k.split(':')[0]\n",
    "                    d['key'] = ':'.join(k.split(':')[1:])\n",
    "                else:\n",
    "                    d['type'] = 'regular'\n",
    "                    d['key'] = k\n",
    "                    \n",
    "            if  k == \"addr:street\":\n",
    "                d['value'] = update_street(tag.attrib['v'], mapping_street)\n",
    "            elif  k == \"addr:state\":\n",
    "                d['value'] = update_state(tag.attrib['v'], mapping_state)\n",
    "            elif  k == \"addr:postcode\":\n",
    "                d['value'] = update_zipcode(tag.attrib['v'])\n",
    "            else:\n",
    "                d['value'] = tag.attrib['v']\n",
    "            tags.append(d)\n",
    "        \n",
    "        index = 0\n",
    "        for tag in element.iter(\"nd\"):\n",
    "            d={}\n",
    "            d['id'] = way_attribs['id']\n",
    "            d['node_id'] = tag.attrib['ref']\n",
    "            d['position'] = index\n",
    "            way_nodes.append(d)\n",
    "            index +=1\n",
    "        return {'way': way_attribs, 'way_nodes': way_nodes, 'way_tags': tags}\n",
    "\n",
    "def get_element(osm_file, tags=('node', 'way', 'relation')):\n",
    "    \"\"\"Yield element if it is the right type of tag\"\"\"\n",
    "\n",
    "    context = ET.iterparse(osm_file, events=('start', 'end'))\n",
    "    _, root = next(context)\n",
    "    for event, elem in context:\n",
    "        if event == 'end' and elem.tag in tags:\n",
    "            yield elem\n",
    "            root.clear()\n",
    "\n",
    "def validate_element(element, validator, schema=SCHEMA):\n",
    "    \"\"\"Raise ValidationError if element does not match schema\"\"\"\n",
    "    if validator.validate(element, schema) is not True:\n",
    "        field, errors = next(validator.errors.iteritems())\n",
    "        message_string = \"\\nElement of type '{0}' has the following errors:\\n{1}\"\n",
    "        error_string = pprint.pformat(errors)\n",
    "        \n",
    "        raise Exception(message_string.format(field, error_string))\n",
    "\n",
    "class UnicodeDictWriter(csv.DictWriter, object):\n",
    "    \"\"\"Extend csv.DictWriter to handle Unicode input\"\"\"\n",
    "\n",
    "    def writerow(self, row):\n",
    "        super(UnicodeDictWriter, self).writerow({\n",
    "            k: (v.encode('utf-8') if isinstance(v, unicode) else v) for k, v in row.iteritems()\n",
    "        })\n",
    "\n",
    "    def writerows(self, rows):\n",
    "        for row in rows:\n",
    "            self.writerow(row)\n",
    "\n",
    "def process_map(file_in, validate):\n",
    "    \"\"\"Iteratively process each XML element and write to csv(s)\"\"\"\n",
    "\n",
    "    with codecs.open(NODES_PATH, 'w') as nodes_file, \\\n",
    "         codecs.open(NODE_TAGS_PATH, 'w') as nodes_tags_file, \\\n",
    "         codecs.open(WAYS_PATH, 'w') as ways_file, \\\n",
    "         codecs.open(WAY_NODES_PATH, 'w') as way_nodes_file, \\\n",
    "         codecs.open(WAY_TAGS_PATH, 'w') as way_tags_file:\n",
    "\n",
    "        nodes_writer = UnicodeDictWriter(nodes_file, NODE_FIELDS)\n",
    "        node_tags_writer = UnicodeDictWriter(nodes_tags_file, NODE_TAGS_FIELDS)\n",
    "        ways_writer = UnicodeDictWriter(ways_file, WAY_FIELDS)\n",
    "        way_nodes_writer = UnicodeDictWriter(way_nodes_file, WAY_NODES_FIELDS)\n",
    "        way_tags_writer = UnicodeDictWriter(way_tags_file, WAY_TAGS_FIELDS)\n",
    "\n",
    "        nodes_writer.writeheader()\n",
    "        node_tags_writer.writeheader()\n",
    "        ways_writer.writeheader()\n",
    "        way_nodes_writer.writeheader()\n",
    "        way_tags_writer.writeheader()\n",
    "\n",
    "        validator = cerberus.Validator()\n",
    "\n",
    "        for element in get_element(file_in, tags=('node', 'way')):\n",
    "            el = shape_element(element)\n",
    "            if el:\n",
    "                if validate is True:\n",
    "                    validate_element(el, validator)\n",
    "\n",
    "                if element.tag == 'node':\n",
    "                    nodes_writer.writerow(el['node'])\n",
    "                    node_tags_writer.writerows(el['node_tags'])\n",
    "                elif element.tag == 'way':\n",
    "                    ways_writer.writerow(el['way'])\n",
    "                    way_nodes_writer.writerows(el['way_nodes'])\n",
    "                    way_tags_writer.writerows(el['way_tags'])\n",
    "\n",
    "process_map(OSM_PATH, validate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create SQL DB from CSV files\n",
    "### Table for nodes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(30730952, 42.3678097, -71.0218711, 'wambag', 326503, 2, 14335103),\n",
      " (30730953, 42.3677364, -71.0218568, 'wambag', 326503, 2, 14335103),\n",
      " (30730954, 42.3676084, -71.0218168, 'wambag', 326503, 2, 14335103),\n",
      " (30730955, 42.3675229, -71.0218486, 'wambag', 326503, 2, 14335103),\n",
      " (30730956, 42.3674548, -71.0218865, 'wambag', 326503, 2, 14335103)]\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import sqlite3\n",
    "\n",
    "# Creates the SQL database\n",
    "con = sqlite3.connect('boston_massachusetts.db')\n",
    "con.text_factory = str\n",
    "cur = con.cursor()\n",
    "\n",
    "# Creates the \"nodes\" table in the database\n",
    "#cur.execute('''DROP TABLE nodes;''')\n",
    "cur.execute('''CREATE TABLE nodes (\n",
    "    id INTEGER PRIMARY KEY NOT NULL,\n",
    "    lat REAL,\n",
    "    lon REAL,\n",
    "    user TEXT,\n",
    "    uid INTEGER,\n",
    "    version INTEGER,\n",
    "    changeset INTEGER,\n",
    "    timestamp TEXT);''') \n",
    "\n",
    "# Inserting values to the \"nodes\" table from CSV files\n",
    "with open ('nodes.csv', 'rb') as table:\n",
    "    dicts = csv.DictReader(table)\n",
    "    to_db = ((i['id'], i['lat'],i['lon'],i['user'],i['uid'],i['version'],i['changeset'],i['timestamp']) for i in dicts) \n",
    "    cur.executemany(\"INSERT INTO nodes (id, lat, lon, user, uid, version, changeset, timestamp) VALUES (?,?,?,?,?,?,?,?);\", to_db)\n",
    "con.commit()\n",
    "#QUERY = '''PRAGMA table_info(nodes)'''\n",
    "QUERY = '''SELECT id, lat, lon, user, uid, version, changeset FROM nodes LIMIT 5;'''\n",
    "rows = cur.execute(QUERY).fetchall()\n",
    "#print(rows)\n",
    "pprint.pprint(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table for nodes_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(31419556, 'name', 'Firebrand Saints', 'regular'),\n",
      " (31419556, 'amenity', 'restaurant', 'regular'),\n",
      " (31419650,\n",
      "  'url',\n",
      "  'http://www.mbta.com/schedules_and_maps/subway/lines/stations/?stopId=12412',\n",
      "  'regular'),\n",
      " (31419650, 'name', 'Kendall/MIT', 'regular'),\n",
      " (31419650, 'railway', 'subway_entrance', 'regular')]\n"
     ]
    }
   ],
   "source": [
    "# Creates the \"nodes_tags\" table in the database\n",
    "#cur.execute('''DROP TABLE nodes_tags;''')\n",
    "cur.execute('''CREATE TABLE nodes_tags (\n",
    "    id INTEGER,\n",
    "    key TEXT,\n",
    "    value TEXT,\n",
    "    type TEXT,\n",
    "    FOREIGN KEY (id) REFERENCES nodes(id));''') \n",
    "\n",
    "# Inserting values to the \"nodes_tags\" table from CSV files\n",
    "with open ('nodes_tags.csv', 'rb') as table:\n",
    "    dicts = csv.DictReader(table)\n",
    "    to_db = ((i['id'], i['key'],i['value'],i['type']) for i in dicts)  \n",
    "    cur.executemany(\"INSERT INTO nodes_tags (id, key,value,type) VALUES (?,?,?,?);\", to_db)\n",
    "con.commit()\n",
    "#QUERY = '''PRAGMA table_info(nodes_tags)'''\n",
    "QUERY = '''SELECT * FROM nodes_tags LIMIT 5;'''\n",
    "rows = cur.execute(QUERY).fetchall()\n",
    "#print(rows)\n",
    "pprint.pprint(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table for ways\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(4790718, 'JessAk71', 381909, '3', 6963203, '2011-01-14T00:06:42Z'),\n",
      " (4790735, 'claysmalley', 119881, '4', 9069918, '2011-08-19T22:12:46Z'),\n",
      " (4824114, 'effektz', 3983283, '20', 39519438, '2016-05-23T20:54:09Z'),\n",
      " (4824115, 'ezr2', 1733549, '13', 32296955, '2015-06-29T23:22:38Z'),\n",
      " (4824116, 'effektz', 3983283, '24', 39518613, '2016-05-23T20:16:38Z')]\n"
     ]
    }
   ],
   "source": [
    "# Creates the \"ways\" table in the database\n",
    "#cur.execute('''DROP TABLE ways;''')\n",
    "cur.execute('''CREATE TABLE ways (\n",
    "    id INTEGER PRIMARY KEY NOT NULL,\n",
    "    user TEXT,\n",
    "    uid INTEGER,\n",
    "    version TEXT,\n",
    "    changeset INTEGER,\n",
    "    timestamp TEXT);''') \n",
    "\n",
    "# Inserting values to the \"ways\" table from CSV files\n",
    "with open ('ways.csv', 'rb') as table:\n",
    "    dicts = csv.DictReader(table)\n",
    "    to_db = ((i['id'], i['user'],i['uid'],i['version'],i['changeset'],i['timestamp']) for i in dicts)  \n",
    "    cur.executemany(\"INSERT INTO ways (id, user,uid,version,changeset,timestamp) VALUES (?,?,?,?,?,?);\", to_db)\n",
    "con.commit()\n",
    "#QUERY = '''PRAGMA table_info(ways)'''\n",
    "QUERY = '''SELECT * FROM ways LIMIT 5;'''\n",
    "rows = cur.execute(QUERY).fetchall()\n",
    "#print(rows)\n",
    "pprint.pprint(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table for ways_tags\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(4790718, 'foot', 'yes', 'regular'),\n",
      " (4790718, 'name', 'Skybridge to Hilton', 'regular'),\n",
      " (4790718, 'layer', '1', 'regular'),\n",
      " (4790718, 'bridge', 'yes', 'regular'),\n",
      " (4790718, 'highway', 'footway', 'regular')]\n"
     ]
    }
   ],
   "source": [
    "# Creates the \"ways_tags\" table in the database\n",
    "#cur.execute('''DROP TABLE ways_tags;''')\n",
    "cur.execute('''CREATE TABLE ways_tags (\n",
    "    id INTEGER NOT NULL,\n",
    "    key TEXT NOT NULL,\n",
    "    value TEXT NOT NULL,\n",
    "    type TEXT,\n",
    "    FOREIGN KEY (id) REFERENCES ways(id));''') \n",
    "\n",
    "# Inserting values to the \"ways_tags\" table from CSV files\n",
    "with open ('ways_tags.csv', 'rb') as table:\n",
    "    dicts = csv.DictReader(table)\n",
    "    to_db = ((i['id'], i['key'],i['value'],i['type']) for i in dicts)  \n",
    "    cur.executemany(\"INSERT INTO ways_tags(id, key,value,type) VALUES (?,?,?,?);\", to_db)\n",
    "con.commit()\n",
    "#QUERY = '''PRAGMA table_info(ways_tags)'''\n",
    "QUERY = '''SELECT * FROM ways_tags LIMIT 5;'''\n",
    "rows = cur.execute(QUERY).fetchall()\n",
    "#print(rows)\n",
    "pprint.pprint(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table for ways_nodes \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(4790718, 30730967, 0),\n",
      " (4790718, 30730968, 1),\n",
      " (4790718, 325383304, 2),\n",
      " (4790735, 1404985339, 0),\n",
      " (4790735, 1404985374, 1)]\n"
     ]
    }
   ],
   "source": [
    "# Creates the \"ways_nodes\" table in the database\n",
    "#cur.execute('''DROP TABLE ways_nodes;''')\n",
    "cur.execute('''CREATE TABLE ways_nodes (\n",
    "    id INTEGER NOT NULL,\n",
    "    node_id INTEGER NOT NULL,\n",
    "    position INTEGER NOT NULL,\n",
    "    FOREIGN KEY (id) REFERENCES ways(id),\n",
    "    FOREIGN KEY (node_id) REFERENCES nodes(id));''') \n",
    "\n",
    "# Inserting values to the \"ways_nodes\" table from CSV files\n",
    "with open ('ways_nodes.csv', 'rb') as table:\n",
    "    dicts = csv.DictReader(table)\n",
    "    to_db = ((i['id'], i['node_id'],i['position']) for i in dicts)  \n",
    "    cur.executemany(\"INSERT INTO ways_nodes(id, node_id,position) VALUES (?,?,?);\", to_db)\n",
    "con.commit()\n",
    "#QUERY = '''PRAGMA table_info(ways_nodes)'''\n",
    "QUERY = '''SELECT * FROM ways_nodes LIMIT 5;'''\n",
    "rows = cur.execute(QUERY).fetchall()\n",
    "#print(rows)\n",
    "pprint.pprint(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Overview \n",
    "### File Sizes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file 'nodes.csv' is 151.8 MB\n",
      "file 'nodes_tags.csv' is 16.9 MB\n",
      "file 'ways.csv' is 20.0 MB\n",
      "file 'ways_tags.csv' is 21.5 MB\n",
      "file 'ways_nodes.csv' is 52.3 MB\n",
      "file 'boston_massachusetts_sample.db' is 2.3 MB\n",
      "file 'boston_massachusetts.osm' is 414.2 MB\n"
     ]
    }
   ],
   "source": [
    "# Creates a list of file sizes\n",
    "import os\n",
    "files_lst = ['nodes.csv', 'nodes_tags.csv', 'ways.csv', 'ways_tags.csv', 'ways_nodes.csv',\n",
    "             'boston_massachusetts_sample.db', 'boston_massachusetts.osm']\n",
    "for i in files_lst: \n",
    "    print \"file {!r} is {!s} MB\".format(i,round(os.path.getsize(i)/(1024*1024.0),1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of nodes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1931442\n"
     ]
    }
   ],
   "source": [
    "# Queries the Number of nodes\n",
    "QUERY=('''SELECT COUNT(*) FROM nodes;''')\n",
    "rows = cur.execute(QUERY).fetchall()\n",
    "print rows[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of ways "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "309066\n"
     ]
    }
   ],
   "source": [
    "# Queries the Number of ways\n",
    "QUERY=('''SELECT COUNT(*) FROM ways;''')\n",
    "rows = cur.execute(QUERY).fetchall()\n",
    "print rows[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration\n",
    "This section contains basic statistics about the dataset, the SQL queries used to gather them, and some additional ideas about the data in context.\n",
    "### Top Postal Codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "02139 : 431\n",
      "02135 : 268\n",
      "02130 : 182\n",
      "02134 : 162\n",
      "02474 : 133\n",
      "02144 : 117\n",
      "02138 : 97\n",
      "02114 : 86\n",
      "02143 : 67\n",
      "02145 : 64\n"
     ]
    }
   ],
   "source": [
    "# Queries the Top 10 Postal Codes by count\n",
    "QUERY=('''SELECT tags.value, COUNT(*) as count \n",
    "        FROM (SELECT * FROM nodes_tags \n",
    "        UNION ALL \n",
    "        SELECT * FROM ways_tags) tags\n",
    "        WHERE tags.key='postcode'\n",
    "        GROUP BY tags.value\n",
    "        ORDER BY count DESC LIMIT 10;''')\n",
    "rows = cur.execute(QUERY).fetchall()\n",
    "for k, v in rows: print k, ':', v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sort cities by count, descending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Boston : 949\n",
      "Cambridge : 556\n",
      "Malden : 413\n",
      "1 : 411\n",
      "Arlington : 285\n",
      "Somerville : 245\n",
      "Jamaica Plain : 96\n",
      "2 : 92\n",
      "Quincy : 55\n",
      "15 : 52\n"
     ]
    }
   ],
   "source": [
    "# Queries the Top 10 cities by count\n",
    "QUERY=('''SELECT tags.value, COUNT(*) as count \n",
    "FROM (SELECT * FROM nodes_tags UNION ALL \n",
    "      SELECT * FROM ways_tags) tags\n",
    "WHERE tags.key LIKE '%city'\n",
    "GROUP BY tags.value\n",
    "ORDER BY count DESC\n",
    "LIMIT 10;''')\n",
    "rows = cur.execute(QUERY).fetchall()\n",
    "for k, v in rows: print k, ':', v\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of  unique users "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1295\n"
     ]
    }
   ],
   "source": [
    "# Queries the Number of unique users\n",
    "QUERY=('''SELECT COUNT(DISTINCT(e.uid))          \n",
    "FROM (SELECT uid FROM nodes UNION ALL SELECT uid FROM ways) e;''')\n",
    "rows = cur.execute(QUERY).fetchall()\n",
    "print rows[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 10 contributing users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "crschmidt : 1202965\n",
      "jremillard-massgis : 430112\n",
      "OceanVortex : 92067\n",
      "wambag : 80064\n",
      "morganwahl : 69535\n",
      "ryebread : 67063\n",
      "MassGIS Import : 63277\n",
      "ingalls_imports : 32461\n",
      "Ahlzen : 27154\n",
      "mapper999 : 14967\n"
     ]
    }
   ],
   "source": [
    "# Queries Top 10 contributing users\n",
    "QUERY=('''SELECT e.user, COUNT(*) as num\n",
    "FROM (SELECT user FROM nodes UNION ALL SELECT user FROM ways) e\n",
    "GROUP BY e.user\n",
    "ORDER BY num DESC\n",
    "LIMIT 10;''')\n",
    "rows = cur.execute(QUERY).fetchall()\n",
    "for k, v in rows: print k, ':', v\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of users appearing only once (having 1 post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "355\n"
     ]
    }
   ],
   "source": [
    "# Queries the Number of users appearing only once (having 1 post)\n",
    "con = sqlite3.connect('boston_massachusetts.db')\n",
    "con.text_factory = str\n",
    "cur = con.cursor()\n",
    "QUERY=('''SELECT COUNT(*) \n",
    "FROM\n",
    "    (SELECT e.user, COUNT(*) as num\n",
    "     FROM (SELECT user FROM nodes UNION ALL SELECT user FROM ways) e\n",
    "     GROUP BY e.user\n",
    "     HAVING num=1)  u;''')\n",
    "rows = cur.execute(QUERY).fetchall()\n",
    "print rows[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 10 appearing amenities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bench : 1060\n",
      "restaurant : 602\n",
      "school : 509\n",
      "place_of_worship : 287\n",
      "library : 280\n",
      "bicycle_parking : 273\n",
      "cafe : 248\n",
      "fast_food : 184\n",
      "bicycle_rental : 138\n",
      "post_box : 114\n"
     ]
    }
   ],
   "source": [
    "# Queries Top 10 appearing amenities\n",
    "QUERY=('''SELECT value, COUNT(*) as num\n",
    "FROM nodes_tags\n",
    "WHERE key='amenity'\n",
    "GROUP BY value\n",
    "ORDER BY num DESC\n",
    "LIMIT 10;''')\n",
    "rows = cur.execute(QUERY).fetchall()\n",
    "for k, v in rows: print k, ':', v\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Biggest religion (no surprise here)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "christian : 255\n"
     ]
    }
   ],
   "source": [
    "# Queries the Biggest religion\n",
    "QUERY=('''SELECT nodes_tags.value, COUNT(*) as num\n",
    "FROM nodes_tags \n",
    "    JOIN (SELECT DISTINCT(id) FROM nodes_tags WHERE value='place_of_worship') i\n",
    "    ON nodes_tags.id=i.id\n",
    "WHERE nodes_tags.key='religion'\n",
    "GROUP BY nodes_tags.value\n",
    "ORDER BY num DESC\n",
    "LIMIT 1;''')\n",
    "rows = cur.execute(QUERY).fetchall()\n",
    "for k, v in rows: print k, ':', v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most popular cuisines\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pizza : 38\n",
      "american : 35\n",
      "italian : 31\n",
      "chinese : 29\n",
      "mexican : 27\n",
      "indian : 21\n",
      "thai : 19\n",
      "asian : 13\n",
      "japanese : 12\n",
      "regional : 12\n"
     ]
    }
   ],
   "source": [
    "# Queries Top 10 popular cuisines\n",
    "QUERY=('''SELECT nodes_tags.value, COUNT(*) as num\n",
    "FROM nodes_tags \n",
    "    JOIN (SELECT DISTINCT(id) FROM nodes_tags WHERE value='restaurant') i\n",
    "    ON nodes_tags.id=i.id\n",
    "WHERE nodes_tags.key='cuisine'\n",
    "GROUP BY nodes_tags.value\n",
    "ORDER BY num DESC\n",
    "LIMIT 10;''')\n",
    "rows = cur.execute(QUERY).fetchall()\n",
    "for k, v in rows: print k, ':', v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Ideas\n",
    "One of the most useful attribute of all tags is the \"building\" key. People are looking up and searching for different types of buildings very often and on a daily basis. Having the OSM database updated with such an information is necessary. As of now, everybody's using \"Yelp\" or \"Google maps\" since they have a vast data on the type of buildings. \n",
    "\n",
    "Looking at Boston OSM file, I found out that only a handful of buildings (less than 400) have assigned a value for the \"building\" key where most of them (334) have assigned \"yes\" as the value which is incorrect and useless.\n",
    "\n",
    "As a suggestion, I think OpenStreetMap.org can implement a structure that ask users and refer them to a pre-defined table for choosing the type of buildings as an input.\n",
    "Moreover, this table can provide different examples for each type of buildings and helps users to choose more carefully and create more accurate database. \n",
    "\n",
    "The benefits of such an improvement in database is obvious (as I explained above) but it's not an easy process and some problems can occur such as:\n",
    "- If we use a complicated table, users may prefer to skip inputting such an information, or use a random inaccurate inputs like the case of inputting 'yes'. \n",
    "- Most of the buildings have multiple usage and can be assigned with different key values. This again can confuse and frustrate the users.\n",
    "\n",
    "I think we should look for some rewards or motivational procedures so users spend some time to update the database in this way.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes : 334\n",
      "entrance : 29\n",
      "commercial : 7\n",
      "school : 6\n",
      "retail : 3\n",
      "apartments : 2\n",
      "office : 2\n",
      "residential : 2\n",
      "university : 2\n",
      "brewery : 1\n",
      "chapel : 1\n",
      "church : 1\n",
      "dormitory : 1\n",
      "industrial : 1\n",
      "lot : 1\n",
      "public : 1\n",
      "warehouse : 1\n"
     ]
    }
   ],
   "source": [
    "# Queries all types of buildings\n",
    "QUERY=('''SELECT value, COUNT(*) as num\n",
    "FROM nodes_tags\n",
    "WHERE key='building'\n",
    "GROUP BY value\n",
    "ORDER BY num DESC;''')\n",
    "rows = cur.execute(QUERY).fetchall()\n",
    "for k, v in rows: print k, ':', v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "In this project, we tried to audit and clean the most popular tags (**street**, **state** and **postcode**) in **Boston OSM file** and obviously lots of cleaning processes are remained which are out of scope of this project. However, the significant number of users and their contributions to OpenStreetMap.org is very promising and I think we will have most of the OSM data cleaned in the near future. "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:DAND]",
   "language": "python",
   "name": "conda-env-DAND-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
